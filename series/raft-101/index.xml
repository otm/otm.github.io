<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>otm.io</title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>https://otm.github.io/series/raft-101/</link>
    <language>en-us</language>
    <author>Nils Lagerkvist</author>
    <copyright>2015 Nils Lagerkvist</copyright>
    <updated>Sat, 30 May 2015 15:28:10 CEST</updated>
    
    
    <item>
      <title>Raft: A First Implementation</title>
      <link>https://otm.github.io/2015/05/raft-a-first-implementation/</link>
      <pubDate>Sat, 30 May 2015 15:28:10 CEST</pubDate>
      <author>Nils Lagerkvist</author>
      <guid>https://otm.github.io/2015/05/raft-a-first-implementation/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;&lt;em&gt;tl;dr In this part we will implement a very simplistic cluster node using the etcd raft implementation. And with the node implementation we will start up a small cluster.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The complete source to this post can be found at &lt;a href=&#34;http://github.com/otm/raft-part-1&#34;&gt;http://github.com/otm/raft-part-1&lt;/a&gt;, and documentation for the etcd raft at &lt;a href=&#34;https://godoc.org/github.com/coreos/etcd/raft&#34;&gt;https://godoc.org/github.com/coreos/etcd/raft&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;creating-a-basic-node:91623121bcc5f99f9298f5ee8624ca3c&#34;&gt;Creating a Basic Node&lt;/h3&gt;

&lt;p&gt;Clusters are built by nodes, so lets start of by defining a node. Initially the node only needs to keep track of the raft and the persistent storage. Below is a minimal implementation of the node type and its constructor function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (
  &amp;quot;github.com/coreos/etcd/raft&amp;quot;
  &amp;quot;github.com/coreos/etcd/raft/raftpb&amp;quot;
  &amp;quot;golang.org/x/net/context&amp;quot;
)

  const hb = 1

  type node struct {
    // id is the node id in the cluster
    id        unint64

    // the raft that the cluster node will use
    // this includes the WAL
    raft      raft.Node

    // the raft configuration
    cfg      *raft.Config

    // pstore is a fake implementation of a persistent storage
    // that will be used side-by-side with the WAL in the raft
    pstore    map[string]string
  }

  func newNode(id int, peers []raft.Peer) *node {
    n := &amp;amp;node{
      id: id,
      cfg: &amp;amp;raft.Config{
        // ID in the etcd raft
        ID:              uint64(id),

        // ElectionTick controls the time before a new election starts
        ElectionTick:    10 * hb,

        // HeartbeatTick controls the time between heartbeats
        HeartbeatTick:   hb,

        // Storage contains the log
        Storage:         raft.NewMemoryStorage(),

        MaxSizePerMsg:   math.MaxUint16,
        MaxInflightMsgs: 256,
      },
      pstore: make(map[string]string),
    }

    n.raft = raft.StartNode(n.cfg, peers)

    return n
  }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the node comes some responsibilities:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read from &lt;code&gt;Node.Ready()&lt;/code&gt; channel and process the updates.&lt;/li&gt;
&lt;li&gt;All persisted log entries must be made available via an implementation of the Storage interface. This can be solved by using the provided MemoryStorage type, if it is repopulated upon restart; or by implementing a disked backed implementation. In this example we will not implement this part.&lt;/li&gt;
&lt;li&gt;Call &lt;code&gt;Node.Step()&lt;/code&gt; when receiving a message from another node.&lt;/li&gt;
&lt;li&gt;Call &lt;code&gt;Node.Tick()&lt;/code&gt; at regular intervals. Internally the raft time is represented by an abstract &lt;em&gt;tick&lt;/em&gt;, that controls two important timeouts, the heartbeat and election timeout.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So the state machine main loop will look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (n *node) run() {
  n.ticker = time.Tick(time.Second)
  for {
    select {
    case &amp;lt;-n.ticker:
      // Point (4) above
      n.raft.Tick()
    case rd := &amp;lt;-n.raft.Ready():
      // Point (2) above
    case &amp;lt;-n.done:
      return
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;handling-raft-ready-events:91623121bcc5f99f9298f5ee8624ca3c&#34;&gt;Handling Raft Ready Events&lt;/h3&gt;

&lt;p&gt;Above a large part of the control loop is left out, that is the processing of updates from the Node.Ready() channel. When receiving a message there are four important tasks:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Write &lt;code&gt;HardState&lt;/code&gt;, &lt;code&gt;Entries&lt;/code&gt; and &lt;code&gt;Snapshot&lt;/code&gt; to persistent storage. &lt;strong&gt;Note!&lt;/strong&gt; When writing to storage it is important to check the Entry Index (i). If previously persisted entries with Index &amp;gt;= i exist, those entries needs to be discarded. For instance this can happen if we get a cluster split with the leader in the minority part; because then the cluster can advance in the other part.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Send all messages to the nodes named in the &lt;code&gt;To&lt;/code&gt; field. &lt;strong&gt;Note!&lt;/strong&gt; It is important to not send any messages until:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the latest HardState has been persisted&lt;/li&gt;
&lt;li&gt;all Entries from previous batch have been persisted (messages from the current batch can be sent and persisted in parallel)&lt;/li&gt;
&lt;li&gt;call &lt;code&gt;Node.ReportSnapshot()&lt;/code&gt; if any message has type &lt;code&gt;MsgSnap&lt;/code&gt; and the snapshot has been sent&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Apply &lt;code&gt;Snapshot&lt;/code&gt; and &lt;code&gt;CommitedEntries&lt;/code&gt; to the state machine. If any committed Entry has the type &lt;code&gt;EntryConfChange&lt;/code&gt; call &lt;code&gt;Node.ApplyConfChange&lt;/code&gt; to actually apply it to the node. The configuration change can be canceled at this point by setting the &lt;code&gt;NodeId&lt;/code&gt; field to zero before calling &lt;code&gt;ApplyConfChange&lt;/code&gt;. Either way, &lt;code&gt;ApplyConfChange&lt;/code&gt; must be called; and the decision to cancel must be based solely on the state machine and not on external information, for instance observed health state of a node.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Call Node.Advance() to signal readiness for the next batch. This can be done any time after step 1 is finished. &lt;strong&gt;Note!&lt;/strong&gt; All updates must be processed in the order they were received by &lt;code&gt;Node.Ready()&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The four tasks above can be done in parallel as long as all notices above are fulfilled. In the example below &lt;code&gt;rd&lt;/code&gt; is of the type &lt;code&gt;raft.Ready&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;n.saveToStorage(rd.HardState, rd.Entries, rd.Snapshot) // (1)
n.send(rd.Messages) // (2)
if !raft.IsEmptySnap(rd.Snapshot) {
  n.processSnapshot(rd.Snapshot)  //(3)
}
for _, entry := range rd.CommittedEntries {
  n.process(entry) // (3)
  if entry.Type == raftpb.EntryConfChange {
    var cc raftpb.ConfChange
    cc.Unmarshal(entry.Data)
    n.node.ApplyConfChange(cc) // (3)
  }
}
n.raft.Advance() // (4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below is the &lt;code&gt;raft.Ready&lt;/code&gt; type, please note that &lt;code&gt;pb&lt;/code&gt; is actually &lt;code&gt;raftpb&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Ready struct {
    // The current volatile state of a Node.
    // SoftState will be nil if there is no update.
    // It is not required to consume or store SoftState
    // It is useful for logging and debugging
    *SoftState

    // The current state of a Node to be saved to stable storage BEFORE
    // Messages are sent.
    // HardState will be equal to empty state if there is no update.
    pb.HardState

    // Entries specifies entries to be saved to stable storage BEFORE
    // Messages are sent.
    Entries []pb.Entry

    // Snapshot specifies the snapshot to be saved to stable storage.
    Snapshot pb.Snapshot

    // CommittedEntries specifies entries to be committed to a
    // store/state-machine. These have previously been committed to stable
    // store.
    CommittedEntries []pb.Entry

    // Messages specifies outbound messages to be sent AFTER Entries are
    // committed to stable storage.
    // If it contains a MsgSnap message, the application MUST report back to raft
    // when the snapshot has been received or has failed by calling ReportSnapshot.
    Messages []pb.Message
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;save-to-storage:91623121bcc5f99f9298f5ee8624ca3c&#34;&gt;Save To Storage&lt;/h4&gt;

&lt;p&gt;Saving to storage is not so advanced in a simplistic example like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Append the entries - this is basically the message to be committed&lt;/li&gt;
&lt;li&gt;Set the hard state - this will commit the message&lt;/li&gt;
&lt;li&gt;Apply the snapshot - this overwrites the storage with the given snapshot&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (n *node) saveToStorage(hardState raftpb.HardState, entries []raftpb.Entry, snapshot raftpb.Snapshot) {
  n.store.Append(entries)

  if !raft.IsEmptyHardState(hardState) {
    n.store.SetHardState(hardState)
  }

  if !raft.IsEmptySnap(snapshot) {
    n.store.ApplySnapshot(snapshot)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;send:91623121bcc5f99f9298f5ee8624ca3c&#34;&gt;Send&lt;/h4&gt;

&lt;p&gt;For now the RPC will be simulated with a global map which holds the nodes. With the send function there is also a matching receive function. Note, the &lt;code&gt;receive()&lt;/code&gt; function calls &lt;code&gt;raft.Step()&lt;/code&gt;, and that is crucial to advance the state machine.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (n *node) send(messages []raftpb.Message) {
  for _, m := range messages {
    // Inspect the message (just for fun)
    log.Println(raft.DescribeMessage(m, nil))

    // send message to other node
    nodes[m.To].receive(n.ctx, m)
  }
}

func (n *node) receive(ctx context.Context, message raftpb.Message) {
  n.raft.Step(ctx, message)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;process-raft-entry:91623121bcc5f99f9298f5ee8624ca3c&#34;&gt;Process Raft Entry&lt;/h4&gt;

&lt;p&gt;If &lt;code&gt;entry.Type&lt;/code&gt; of the &lt;code&gt;raftpb.Entry&lt;/code&gt; is &lt;code&gt;raftpb.EntryNormal&lt;/code&gt; the message should be processed. The message will be encoded in &lt;code&gt;entry.Data&lt;/code&gt;. The protocol below is very simple, and is a string on the form &lt;em&gt;key:value&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (n *node) process(entry raftpb.Entry) {

  if entry.Type == raftpb.EntryNormal &amp;amp;&amp;amp; entry.Data != nil {
    log.Println(&amp;quot;normal message:&amp;quot;, string(entry.Data))

    parts := bytes.SplitN(entry.Data, []byte(&amp;quot;:&amp;quot;), 2)
    n.pstore[string(parts[0])] = string(parts[1])
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;process-snapshot:91623121bcc5f99f9298f5ee8624ca3c&#34;&gt;Process Snapshot&lt;/h4&gt;

&lt;p&gt;For now &lt;code&gt;processSnapshot&lt;/code&gt; will only be a dummy implementation. But basically it should only overwrite the the current persistent storage.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (n *node) processSnapshot(snapshot raftpb.Snapshot) {
  log.Printf(&amp;quot;Applying snapshot on %v is not implemenetd yet&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;advance-the-raft:91623121bcc5f99f9298f5ee8624ca3c&#34;&gt;Advance the Raft&lt;/h4&gt;

&lt;p&gt;At this point the only thing left to do is calling &lt;code&gt;Node.Advance()&lt;/code&gt;, this signals that the node is ready for the next batch. Please note that &lt;em&gt;all&lt;/em&gt; updates must be processed in the order they were received from &lt;code&gt;Node.Ready&lt;/code&gt;. In our example it looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;n.raft.Advance()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;using-the-raft:91623121bcc5f99f9298f5ee8624ca3c&#34;&gt;Using the Raft&lt;/h3&gt;

&lt;p&gt;Finally it is time to start up a couple of nodes and connect them to a cluster. Below the nodes are started with predefined peers. Lastly &lt;code&gt;Node.Campaign&lt;/code&gt; is called to start an election campaign; it is not necessary but it saves some time, as we do not need to wait for the election timeout. &lt;strong&gt;Note!&lt;/strong&gt; At this point we do not have a resilient cluster, because if a node is lost there is no way to reach a majority consensus.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;nodes[1] = newNode(1, []raft.Peer{{ID: 1}, {ID: 2}})
go nodes[1].run()

nodes[2] = newNode(2, []raft.Peer{{ID: 1}, {ID: 2}})
go nodes[2].run()

nodes[1].raft.Campaign(nodes[1].ctx)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, all nodes in the cluster can not be known in advance. Below a node is created and added to a running cluster. It is created without any peers and a configuration change is proposed to the cluster. When the configuration change has been committed, the cluster is fault tolerant.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;nodes[3] = newNode(3, []raft.Peer{})
go nodes[3].run()
nodes[2].raft.ProposeConfChange(nodes[2].ctx, raftpb.ConfChange{
  ID:      3,
  Type:    raftpb.ConfChangeAddNode,
  NodeID:  3,
  Context: []byte(&amp;quot;&amp;quot;),
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next up is writing data to the cluster, that is also done by proposing a change to the cluster. Each of the writes are done to different nodes in the cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;nodes[1].node.Propose(nodes[1].ctx, []byte(&amp;quot;key1:value1&amp;quot;))
nodes[2].node.Propose(nodes[2].ctx, []byte(&amp;quot;key2:value2&amp;quot;))
nodes[3].node.Propose(nodes[3].ctx, []byte(&amp;quot;key3:value3&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dumping the data on the nodes reveals if they have been synchronized properly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;for i, node := range nodes {
  fmt.Printf(&amp;quot;** Node %v **\n&amp;quot;, i)
  for k, v := range node.pstore {
    fmt.Printf(&amp;quot;%v = %v&amp;quot;, k, v)
  }
  fmt.Printf(&amp;quot;*************\n&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If a node is added to the cluster, data will be replicated to the newly added node by replaying the log.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>The Raft Algorithm</title>
      <link>https://otm.github.io/2015/05/the-raft-algorithm/</link>
      <pubDate>Wed, 27 May 2015 22:24:16 CEST</pubDate>
      <author>Nils Lagerkvist</author>
      <guid>https://otm.github.io/2015/05/the-raft-algorithm/</guid>
      <description>

&lt;p&gt;The Raft consensus algorithm provides an understandable, easy to use, and generic way to distribute a state machine in a cluster. The Raft is a successor, or an alternative, to an algorithm called Paxos.&lt;/p&gt;

&lt;p&gt;A node in the cluster can have three states: follower, candidate, or leader. Consensus in the cluster is maintained by the leader, which is responsible for log replication. To become the leader a node will change its status from follower to candidate.&lt;/p&gt;

&lt;h3 id=&#34;leader-election:3615e096b6b5dd4aa55f32d60f44baab&#34;&gt;Leader Election&lt;/h3&gt;

&lt;p&gt;The leader in the cluster sends heartbeat messages to the followers. If a follower does not receive a heartbeat within the election timeout it will transition state from follower to candidate. The candidate starts a new election term and increases the &lt;em&gt;term&lt;/em&gt; counter, votes for itself and sends a &lt;em&gt;RequestVote&lt;/em&gt; message to the cluster members. A member node will vote on the first candidate that sends a &lt;em&gt;RequestVote&lt;/em&gt; message, and it will only vote once during a &lt;em&gt;term&lt;/em&gt;. At this point there are three scenarios in the cluster:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The candidate receives a message from a leader with equal or higher &lt;em&gt;term&lt;/em&gt; - and transitions from candidate to flower.&lt;/li&gt;
&lt;li&gt;The candidate receives a majority of the votes - and transitions from candidate to leader.&lt;/li&gt;
&lt;li&gt;The candidate&amp;rsquo;s election times out - and transitions from candidate to follower.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;log-replication:3615e096b6b5dd4aa55f32d60f44baab&#34;&gt;Log Replication&lt;/h3&gt;

&lt;p&gt;All changes go through the leader, leader gets proposal, creates a log entry (uncommited), then replicates the entry to the followers, when a majority of the followers have written the log entry the leader commits it (the leaders state has now changed), the leader notifies the followers that the entry has been committed. At this point the cluster is in consensus.&lt;/p&gt;

&lt;h3 id=&#34;log-replication-1:3615e096b6b5dd4aa55f32d60f44baab&#34;&gt;Log replication&lt;/h3&gt;

&lt;p&gt;Once there is a leader the cluster can move forward. All changes go through the leader, however followers can propose a change to the leader. When the leader is updated it creates a log entry, which is uncommited; the leader replicates the log entry to the followers in an &lt;em&gt;AppendEntries&lt;/em&gt; message. When a majority of followers has written the log entry, the leader will commit the pending entry; the leader state has now changed and notifies the followers that the entry is committed. At this point the cluster is in consensus and has moved forward.&lt;/p&gt;

&lt;h3 id=&#34;cluster-split:3615e096b6b5dd4aa55f32d60f44baab&#34;&gt;Cluster Split&lt;/h3&gt;

&lt;p&gt;Cluster consensus requires at least two nodes; thus three nodes are needed to create a resilient cluster. In case of a cluster split there will be one part that can achieve consensus, and move forward. However, there are two scenarios that are interesting to look into.&lt;/p&gt;

&lt;h4 id=&#34;leader-in-majority-part:3615e096b6b5dd4aa55f32d60f44baab&#34;&gt;Leader in Majority Part&lt;/h4&gt;

&lt;p&gt;The leader is in the part of the cluster that can reach consensus - That part will continue to work and move forward. In the other part a new leader can not be elected as consensus can not be reached. When the cluster split is resolved the log entries will replicate to the follower that was on the &amp;ldquo;wrong&amp;rdquo; side of the split.&lt;/p&gt;

&lt;h4 id=&#34;leader-in-minority-part:3615e096b6b5dd4aa55f32d60f44baab&#34;&gt;Leader in Minority Part&lt;/h4&gt;

&lt;p&gt;The leader is in the part of the cluster that can not reach consensus - Proposed changes to the original leader will be entered in the log and replicated to the followers; however the change can not be committed and that part of the cluster can not move forward. At the same time, on the other side of the cluster a leader election will start, due to the lack of leader, and the term will increment and a leader will be elected. The increment in election term here is important. Now when there is a leader, and enough nodes for consensus, this part of the cluster can start to move forward.&lt;/p&gt;

&lt;p&gt;When the cluster split is resolved there will be two leaders. However, the leader with the lower &lt;em&gt;term&lt;/em&gt; will now have uncommitted entries in the log. When it reseives a message from the leader with a higher &lt;em&gt;term&lt;/em&gt; it will roll back the log entries and start to commit the log entries from the leader with higher &lt;em&gt;term&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Next part will be a practical introduction to the Raft algorithm in etcd.&lt;/p&gt;
</description>
    </item>
    
    
  </channel>
</rss>